runApp('~/Documents/BenProjects/covid19/old_app')
runApp('~/Documents/BenProjects/covid19/old_app')
runApp()
deployApp()
population_data <- read.csv('co-est2019-alldata.csv')
population_data$CTYNAME <- str_replace(population_data$CTYNAME," County","")
population_data %>% filter(CTYNAME=="Cook",STNAME == "Illinois")
## Read in data
covid_data <- read.csv('us-counties.csv')
covid_data$date<-as.Date(covid_data$date)
population_data <- read.csv('co-est2019-alldata.csv')
population_data$CTYNAME <- str_replace(population_data$CTYNAME," County","")
population_data$CTYNAME[population_data$CTYNAME=="New York"&population_data$COUNTY==61]= "New York City"
covid_data <- covid_data %>%
filter(date > as.Date("2020-02-29")) %>%
left_join(population_data %>% select(STNAME,CTYNAME,POPESTIMATE2019),by = c("state"="STNAME","county" = "CTYNAME")) %>%
unite(state_county,c(county,state),sep = ", ",remove = FALSE)
covid_data$population <- covid_data$POPESTIMATE2019
covid_data%>%filter(county=="Cook")
covid_data <- covid_data %>%
mutate(cases_per_capita = cases/population) %>%
mutate(deaths_per_capita = deaths/population) %>%
mutate(death_rate = deaths/cases)
drv <- function(x) c(NA, diff(x))
# first_date <- function(x,y) min(x[y==min(y)],na.rm=T)
case_threshold <- 10 #Controls when to start counting days
first_date <- function(x,y) min(x[y>=case_threshold],na.rm=T)
covid_data <- covid_data %>%
group_by(state_county) %>%
mutate(daily_change_per_capita = drv(cases_per_capita)) %>%
mutate(daily_change = drv(cases)) %>%
mutate(daily_change_deaths_per_capita = drv(deaths_per_capita)) %>%
mutate(daily_change_deaths = drv(deaths)) %>%
mutate(days_since_first_case = as.numeric(date - first_date(date,cases)))
runApp()
covid_data %>% filter(county=="Cook") %>% select(county,state,state_county,population)
this<-"Cook, Illinois"
cdata <- covid_data %>%
filter(state_county %in% list(this))
ggplot(data=cdata,aes(x = reorder(state_county,-population), y = population,fill = state_county)) +
geom_bar(stat = "identity") + scale_y_continuous(labels = comma) +
scale_fill_brewer(type = "qual", palette = "Dark2") +
ylab("Population") + xlab("County")
cdata
View(cdata)
cdata <- covid_data %>% group_by(state_county) %>% summarise(population=population[1])
cdata <- covid_data %>% group_by(state_county) %>% summarise(population=mean(population,na.rm=T)
)
ggplot(data=cdata,aes(x = reorder(state_county,-population), y = population,fill = state_county)) +
geom_bar(stat = "identity") + scale_y_continuous(labels = comma) +
scale_fill_brewer(type = "qual", palette = "Dark2") +
ylab("Population") + xlab("County")
cdata <- covid_data %>% group_by(state_county) %>% summarise(population=mean(population,na.rm=T)) %>%filter(state_county %in% list(this))
ggplot(data=cdata,aes(x = reorder(state_county,-population), y = population,fill = state_county)) +
geom_bar(stat = "identity") + scale_y_continuous(labels = comma) +
scale_fill_brewer(type = "qual", palette = "Dark2") +
ylab("Population") + xlab("County")
runApp()
View(population_data)
View(covid_data)
covid_data %>% filter(state == "New York") %>% select(state, county, fips, population) %>% unique()
covid_data %>% filter(state == "New York") %>% select(state, county, fips, population) %>% unique() %>% arrange(population)
covid_data %>% filter(state == "New York") %>% select(state, county, fips, population) %>% unique() %>% arrange(-population)
deployApp()
## Load Libraries
require(ggplot2)
require(tidyverse)
library(oro.nifti)
library(neurobase)
library(gtools)
library(e1071)
library(ridge)
library(matrixTests)
library(permute)
library(doParallel)
theme_set(theme_classic())
setwd("/cbica/projects/alpraz_EI/scripts/")
extract_voxels <- function(subid,sesid,dataname) {
fname <- sprintf("/cbica/projects/alpraz_EI/data/%s/%s/%s/amygdala_conn_fisherz.nii.gz",dataname,subid,sesid)
mask<-coverage_mask
this_nii <- readnii(fname)
vals <- c(this_nii[mask==1])
return(vals)
}
extract_matrix2 <- function(subid,sesid,atlasname,gsr){
if (gsr == "GSR") {
fname <- sprintf("/cbica/projects/alpraz_EI/data/SDK_TASK_GSR/%s/%s/%s_CC_GSR_000.netcc",subid,sesid,atlasname)
} else if (gsr == "rest_GSR") {
fname <- sprintf("/cbica/projects/alpraz_EI/data/XCP_SDK_NO_TASK_REGRESS_GSR2020-04-29/%s/%s/%s_CC_rest_000.netcc",subid,sesid,atlasname)
} else if (gsr == "rest_noGSR") {
fname <- sprintf("/cbica/projects/alpraz_EI/data/XCP_SDK_NO_TASK_REGRESS/%s/%s/%s_CC_rest_noGSR_000.netcc",subid,sesid,atlasname)
} else {
fname <- sprintf("/cbica/projects/alpraz_EI/data/XCP_SDK_TASK_2020-03-25/%s/%s/%s_CC_noGSR_000.netcc",subid,sesid,atlasname)
}
print(fname)
thisCC <- as.matrix(read.table(fname,header = T,skip = 4))
# thisCC_vector = as.vector(thisCC)
thisCC_vector = thisCC[upper.tri(thisCC,diag = F)]
vals <-thisCC_vector
}
extract_alff <- function(subid,sesid,atlasname,gsr){
if (gsr == "GSR") {
fname <- sprintf("/cbica/projects/alpraz_EI/data/SDK_TASK_GSR/%s/%s/%s_alffZ.1D",subid,sesid,atlasname)
} else {
fname <- sprintf("/cbica/projects/alpraz_EI/data/XCP_SDK_TASK_2020-03-25/%s/%s/%s_alffZ.1D",subid,sesid,atlasname)
}
thisALFF <- as.matrix(read.table(fname,header = T)%>%select(-name))
vals <-thisALFF
}
extract_alff_glasser <- function(subid,sesid,dataname) {
fname <- sprintf("/cbica/projects/alpraz_EI//data/%s/xcpengine/sub-0%s/ses-00%s/task-emotionid/space-MNI152NLin2009cAsym/roiquant/glasser360/sub-0%s_ses-00%s_task-emotionid_space-MNI152NLin2009cAsym_glasser360_mean_alffZ.csv",dataname,subid,sesid,subid,sesid)
thisData<- read.csv(fname)
vals <- as_tibble(thisData)
return(vals)
}
extract_reho_glasser <- function(subid,sesid,dataname) {
fname <- sprintf("/cbica/projects/alpraz_EI//data/%s/xcpengine/sub-0%s/ses-00%s/task-emotionid/space-MNI152NLin2009cAsym/roiquant/glasser360/sub-0%s_ses-00%s_task-emotionid_space-MNI152NLin2009cAsym_glasser360_mean_rehoZ.csv",dataname,subid,sesid,subid,sesid)
thisData<- read.csv(fname)
vals <- as_tibble(thisData)
return(vals)
}
extract_alff_voxels <- function(subid,sesid,dataname) {
fname <- sprintf("/cbica/projects/alpraz_EI/data/%s/%s/%s/alffZ.nii.gz",dataname,subid,sesid)
mask<-coverage_mask
this_nii <- readnii(fname)
vals <- c(this_nii[mask==1])
return(vals)
}
write_W_connectivity <- function(W,atlasname){
#load template matrix (picked 13783 1986 at random)
fname <- sprintf("/cbica/projects/alpraz_EI/data/SDK_TASK_GSR/13783/1986/%s_CC_GSR_000.netcc",atlasname)
thisCC <- as.matrix(read.table(fname,header = T,skip = 4))
thisCC_vector = thisCC[upper.tri(thisCC,diag = F)]
thisCC_vector = W
newCC <- thisCC
newCC[upper.tri(newCC,diag = T)] = 0
newCC[upper.tri(newCC,diag = F)] = thisCC_vector
newCC[lower.tri(newCC)] <- t(newCC)[lower.tri(newCC)]
# Get labels
labels <- read.csv(sprintf('/cbica/projects/alpraz_EI/input/atlases/%s_labels.csv',atlasname),header=F)
mat_labels <- data.frame(nums = colnames(thisCC))
mat_labels$nums <- as.numeric(gsub(mat_labels$nums,pattern="X",replace=""))
mat_labels <- mat_labels %>% left_join(labels,by = c("nums"="V1"))
colnames(newCC)=mat_labels$V2
rownames(newCC)=mat_labels$V2
return(newCC)
}
SVM_2class <- function(df,folds,feature_selection = F,feature_proportion = .1){
# set up folds for CV
if (folds == "LOO") {
num_folds = length(unique(df$subid))
} else {
num_folds = folds
}
foldIdxs <- data.frame(subid=unique(df$subid))
foldIdxs$foldID <- row_number(foldIdxs$subid)
foldIdxs$foldID <- ntile(foldIdxs$foldID,num_folds)
# foldIdxs$subid <- sample(foldIdxs$subid)
pred_data<-setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("subid", "sesid", "drug","model.pred"))
W = matrix(nrow = num_folds,ncol = length(4:dim(df)[2]))
colnames(W)<-colnames(df)[4:dim(df)[2]]
# Loop over folds
for (fold in 1:num_folds) {
#training data
trainingIDs <- as.matrix(foldIdxs %>% filter(foldID != fold) %>% select(subid))
trainingIndex <- df$subid %in% trainingIDs # indices for training subs
trainingData <- trainingData <- df[trainingIndex, 3:dim(df)[2] ] # Training data. Take columns 3:end (Removes subid and sesid).
#testing data
testData <- df[!trainingIndex, 4:dim(df)[2]] # test data. Take columns 4:end (Removes subid sesid drug).
testLabels <- data.frame(df[!trainingIndex,c(1:3) ]) # Labels for test data
#feature extraction if needed
if (feature_selection == TRUE) {
feature_extracted_data <- featureExtraction(trainingData,feature_proportion = feature_proportion)
trainingData <- feature_extracted_data[[1]]
feature_names <- feature_extracted_data[[2]]
testData <- testData %>% select(feature_names)
cat(sprintf("Retained %d features\n",length(feature_names)))
}
# svm
x <- as.matrix(trainingData[, 2:dim(trainingData)[2]])
y <- as.factor(as.matrix(trainingData[,1]))
svm.model <- svm(x =x, y = y,
cost = 100, kernel = "linear",type = "C-classification",scale = F)
svm.pred <- predict(svm.model, as.matrix(testData))
W[fold,colnames(x)] <- t(svm.model$coefs) %*% svm.model$SV
w <- t(svm.model$coefs) %*% svm.model$SV
num_features <- dim(x)[2]
decisionValues <- w %*% t(testData)-svm.model$rho
distance <- decisionValues/norm(w)
testLabels$decisionValues <- t(decisionValues)
testLabels$distance <- t(distance)
testLabels$model.pred = svm.pred
pred_data <- rbind(pred_data,testLabels)
}
svm_results <- list(pred_data,W)
return(svm_results)
}
RF_2class <- function(df,folds,feature_selection = F,feature_proportion = .1){
# set up folds for CV
if (folds == "LOO") {
num_folds = length(unique(df$subid))
} else {
num_folds = folds
}
foldIdxs <- data.frame(subid=unique(df$subid))
foldIdxs$foldID <- row_number(foldIdxs$subid)
foldIdxs$foldID <- ntile(foldIdxs$foldID,num_folds)
# foldIdxs$subid <- sample(foldIdxs$subid)
pred_data<-setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("subid", "sesid", "drug","model.pred"))
W = matrix(nrow = num_folds,ncol = length(4:dim(df)[2]))
colnames(W)<-colnames(df)[4:dim(df)[2]]
# Loop over folds
for (fold in 1:num_folds) {
#training data
trainingIDs <- as.matrix(foldIdxs %>% filter(foldID != fold) %>% select(subid))
trainingIndex <- df$subid %in% trainingIDs # indices for training subs
trainingData <- trainingData <- df[trainingIndex, 3:dim(df)[2] ] # Training data. Take columns 3:end (Removes subid and sesid).
#testing data
testData <- df[!trainingIndex, 4:dim(df)[2]] # test data. Take columns 4:end (Removes subid sesid drug).
testLabels <- data.frame(df[!trainingIndex,c(1:3) ]) # Labels for test data
#feature extraction if needed
if (feature_selection == TRUE) {
feature_extracted_data <- featureExtraction(trainingData,feature_proportion = feature_proportion)
trainingData <- feature_extracted_data[[1]]
feature_names <- feature_extracted_data[[2]]
testData <- testData %>% select(feature_names)
cat(sprintf("Retained %d features\n",length(feature_names)))
}
# randomForest
x <- as.matrix(trainingData[, 2:dim(trainingData)[2]])
y <- as.factor(as.matrix(trainingData[,1]))
rf.model <- randomForest(x = x, y = y,importance = T,mtry = 2*sqrt(dim(x)[2]))
rf.pred <- predict(rf.model, as.matrix(testData))
imp <- importance(rf.model)
W[fold,colnames(x)] <- imp[,"MeanDecreaseAccuracy"]
testLabels$model.pred = rf.pred
# print(sprintf("Fold %d, acc = %1.3f",fold,sum(testLabels$drug==testLabels$model.pred)/nrow(testLabels)))
pred_data <- rbind(pred_data,testLabels)
}
rf_results <- list(pred_data,W)
return(rf_results)
}
run_model <- function(df,folds,feature_selection = F,feature_proportion = .1,permutation_test = F, num_permutations = 1000,type = "svm"){
if (feature_selection == T) {
# cat(sprintf("\nPerforming feature extraction: extracting the top %1.3f features\n",feature_proportion))
}
if (type == "svm") {
print("Performing classification using SVM")
model_results <- SVM_2class(df,folds,feature_selection = feature_selection,feature_proportion = feature_proportion)
}else if (type == "rf") {
print("Performing classification using Random Forest")
model_results <- RF_2class(df,folds,feature_selection = feature_selection,feature_proportion = feature_proportion)
}
pred_data <- model_results[[1]]
W_folds <- model_results[[2]]
num_features <- sum(!is.na(W_folds[1,]))
meanW <- colMeans(W_folds,na.rm=T)
if (feature_selection==T) {
feature_counts <- colSums(!is.na(W_folds))
W <- rbind(meanW,feature_counts)
} else{
W <- meanW
}
accuracy <- sum(pred_data$model.pred==pred_data$drug)/dim(pred_data)[1]
b <- binom.test(sum(pred_data$model.pred==pred_data$drug),dim(pred_data)[1],.5)
if (permutation_test == T) {
print(sprintf("Permuting %d times...",num_permutations))
ptm = proc.time()
cl<-makeCluster(12)
registerDoParallel(cl)
nw <- getDoParWorkers()
# # perms <- shuffleSet(n = dim(df)[1],nset = num_permutations)
perm_acc <- matrix(nrow = num_permutations)
perm_acc = foreach(perm_chunk = idiv(num_permutations,chunks = nw),
.combine=c,
.export = c("featureExtraction","SVM_2class"),
.packages = c("dplyr","e1071","matrixTests")) %dopar% {
pacc = numeric(perm_chunk)
for (p in 1:perm_chunk){
# thisPerm <- perms[p,]
thisDF <- df
# thisDF$drug <- df$drug[thisPerm]
permuted <- df %>% select(subid,drug) %>% group_by(subid) %>% mutate(perm_drug=sample(drug))
thisDF$drug <- permuted$perm_drug
perm_pred_result <- SVM_2class(thisDF,folds,feature_selection = feature_selection,feature_proportion = feature_proportion)
perm_pred_data <- perm_pred_result[[1]]
pacc[p]<-sum(perm_pred_data$model.pred==perm_pred_data$drug)/dim(perm_pred_data)[1]
}
pacc
}
print("done")
stopCluster(cl)
# for (p in 1:num_permutations) {
#   thisDF <- df
#   permuted <- df %>% select(subid,drug) %>% group_by(subid) %>% mutate(perm_drug=sample(drug))
#   thisDF$drug <- permuted$perm_drug
#   perm_pred_data <- SVM_2class(thisDF,folds,feature_selection = feature_selection,feature_proportion = feature_proportion)
#   perm_acc[p] <- sum(perm_pred_data$model.pred==perm_pred_data$drug)/dim(perm_pred_data)[1]
#   # cat(sprintf("Permutation %d: %1.3f\n",p,perm_acc[p]))
# }
print(proc.time()-ptm)
print(ggplot(data = as.data.frame(perm_acc),aes(x = perm_acc))+geom_histogram()+geom_vline(xintercept = accuracy))
perm_p <- sum(perm_acc>accuracy)/length(perm_acc)
cat(sprintf("\nPermutation p-value =  %1.3f\n",perm_p))
b$perm_p <- perm_p
}
print(sprintf("Overall Accuracy: %1.3f; p = %.5f\n\n",sum(pred_data$model.pred==pred_data$drug)/dim(pred_data)[1],b$p.value))
# Group1_Index <- pred_data$drug==1
# Group0_Index <- pred_data$drug==0
# Category_group1 = pred_data$model.pred[Group1_Index]
# Y_group1 = pred_data$decisionValues[Group1_Index]
# Y_group1
# Category_group0 = pred_data$model.pred[Group0_Index]
# Y_group0 = pred_data$decisionValues[Group0_Index]
# Dis_group1_svm = pred_data$distance[Group1_Index]
# Dis_group0_svm = pred_data$distance[Group0_Index]
return(list(b,W,num_features))
}
featureExtraction <- function(trainingData, feature_proportion = .1){
features <- as.matrix(trainingData %>% select(-drug))
drug <- features[trainingData$drug==0,]
placebo <- features[trainingData$drug==1,]
tstats<-col_t_paired(placebo,drug)
newdata=t(tstats[order(-abs(tstats$statistic)),])
keep_features = colnames(newdata)[1:round(feature_proportion*dim(newdata)[2])]
newTrainingData <- trainingData %>% select(drug,keep_features)
return(list(newTrainingData,keep_features))
}
subData <- read.csv('/cbica/projects/alpraz_EI/input/alpraz_sublist_FD.csv')
subInfo <- subData %>% filter(exists==1 & motion_pass==1)
classifier = "svm"
for (GSR in c("rest_noGSR")) {
atlas_acc <- matrix(ncol = 6)
colnames(atlas_acc)=c("accuracy", "p.value", "fe","perm.p","atlas","num_features")
for (atlasname in c("schaefer400x7_aal","schaefer200x7_aal","aal116")){ #,"HarvardOxford","glasser360","gordon333_aal","BN_Atlas_246","aal116","schaefer200x7_aal")) {
print(atlasname)
## Load the data
if (file.exists(sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rds",atlasname,GSR))) {
df <- readRDS(sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rds",atlasname,GSR))
} else {
gm<- subInfo %>%
group_by(subid, sesid) %>%
mutate(mat = list(extract_matrix2(subid,sesid,atlasname,GSR)))
mat <- data.frame(matrix(unlist(gm$mat), nrow=dim(gm[1]), byrow=T))
df <- cbind(gm %>%
ungroup() %>%
select(subid,sesid,drug)
,mat)
print("saving...")
saveRDS(df,file = sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rds",atlasname,GSR))
print("saved")
rm(gm)
}
# #take a look at the paired t-test differences
# features <- as.matrix(df %>% select(-subid,-sesid,-drug))
# drug <- features[df$drug==0,]
# placebo <- features[df$drug==1,]
#
# tstats<-col_t_paired(placebo,drug)
# t_mat <- tstats %>% select(mean.x,mean.y,statistic,pvalue)
# ggplot(t_mat,aes(x=mean.x,y=mean.y,color = statistic)) + geom_point() +
#   scale_color_gradient2(low="blue",high = "red")+geom_abline(slope = 1,intercept = 0) +
#   xlab("placebo") + ylab("drug") + ggtitle(atlasname)
#
# rm(features,drug,placebo,tstats,t_mat)
## Now run the SVM for a set of feature extraction levels.
fe_list = c(1,.9,.5,.2,.1,.05,.01)
fe_list = c(1,.5,.2,.1,.05)
fe_list = c(1,.5,.2,.1,.05)
# fe_list = c(1)
acc = matrix(nrow = length(fe_list),ncol = 5)
result_list = list()
for (n in 1:length(fe_list)) {
num <- fe_list[n]
results <- run_model(df = df,folds = 10,
feature_selection = T,feature_proportion = num,
permutation_test = F,num_permutations = 500,
type = classifier)
results[[4]]=atlasname
b <- results[[1]]
acc[n,1] <- b$estimate
acc[n,2] <- b$p.value
acc[n,3] <- num
acc[n,5] <- results[[3]] #num features
# acc[n,4] <- b$perm_p
# W <- results[[2]]
# if (is.null(dim(W))) {
#   Wmap <- write_W_connectivity(W,atlasname)
# } else if (dim(W)){
#   Wmap <- write_W_connectivity(W[1,],atlasname)
# }
# results[[5]] <- Wmap
# result_list <- c(result_list,list(results))
# # Fmap <- write_W_connectivity(W[2,],atlasname)
# c1 <- corrplot::corrplot(Wmap,is.corr = F, method = "color",na.label = "square",title = sprintf("%s W coefficients",atlasname),
#                          diag = F,type = "upper",tl.cex = .3)
# c2 <- corrplot::corrplot(Fmap,is.corr = F, method = "color",na.label = "square",title = "Feature selection counts")
# cowplot::plot_grid(c(c1,c2))
}
acc <- data.frame(accuracy = acc[,1], p.value = acc[,2], fe = acc[,3],perm.p = acc[,4],atlas = atlasname,num_features = acc[,5])
atlas_acc <- rbind(atlas_acc,acc)
write.csv(atlas_acc,file = sprintf('%s_%s_results_%s.csv',atlasname,GSR,classifier),row.names = F)
# print(ggplot(acc, aes(x = fe,y=accuracy,label=round(p.value,digits = 5))) +
# geom_point() + geom_line() + xlab("F.E. proportion") + ylab("accuracy") +geom_text()+ggtitle(atlasname))
# print(ggplot(acc, aes(x = fe,y=accuracy,label=round(perm.p,digits = 5))) +
# geom_point() + geom_line() + xlab("F.E. proportion") + ylab("accuracy") +geom_text()+ggtitle(atlasname))
}
# get critical values
n=b$parameter #number of observations
p01 <- qbinom(.01/2,size = n,prob = .5,lower.tail = F) +1 # get num correct that corresponds to two-tailed p-value of .01. Add one to get make the value represent prob of x >= p01 instead of x>p01.
p01 = p01/n #convert to proprotion
p001 <- (qbinom(.001/2,size = n,prob = .5,lower.tail = F) +1)/n
acc_plot<-ggplot(atlas_acc, aes(x = num_features,y=accuracy,color = atlas)) +
geom_point() + geom_line() + ylim(c(.5,.8))+
xlab("Num Features") + ylab("accuracy") + ggtitle(sprintf("%s %s",GSR,classifier))+
geom_hline(aes(yintercept = p01,linetype = "p = .01"))+geom_hline(aes(yintercept = p001,linetype="p = .001")) +
scale_linetype_manual(name="significance threshold",values = c(1,2))
print(acc_plot)
ggsave(filename = sprintf('%s_acc_plot_%s.svg',GSR,classifier),plot = acc_plot,device = 'svg')
write.csv(atlas_acc,file = sprintf('%s_results_%s.csv',GSR,classifier),row.names = F)
}
classifier = "svm"
for (GSR in c("rest_noGSR")) {
atlas_acc <- matrix(ncol = 6)
colnames(atlas_acc)=c("accuracy", "p.value", "fe","perm.p","atlas","num_features")
for (atlasname in c("schaefer400x7_aal","schaefer200x7_aal","aal116")){ #,"HarvardOxford","glasser360","gordon333_aal","BN_Atlas_246","aal116","schaefer200x7_aal")) {
print(atlasname)
## Load the data
if (file.exists(sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rdss",atlasname,GSR))) {
df <- readRDS(sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rds",atlasname,GSR))
} else {
gm<- subInfo %>%
group_by(subid, sesid) %>%
mutate(mat = list(extract_matrix2(subid,sesid,atlasname,GSR)))
mat <- data.frame(matrix(unlist(gm$mat), nrow=dim(gm[1]), byrow=T))
df <- cbind(gm %>%
ungroup() %>%
select(subid,sesid,drug)
,mat)
print("saving...")
saveRDS(df,file = sprintf("/cbica/projects/alpraz_EI/input/CorMats/%s_%s.rds",atlasname,GSR))
print("saved")
rm(gm)
}
# #take a look at the paired t-test differences
# features <- as.matrix(df %>% select(-subid,-sesid,-drug))
# drug <- features[df$drug==0,]
# placebo <- features[df$drug==1,]
#
# tstats<-col_t_paired(placebo,drug)
# t_mat <- tstats %>% select(mean.x,mean.y,statistic,pvalue)
# ggplot(t_mat,aes(x=mean.x,y=mean.y,color = statistic)) + geom_point() +
#   scale_color_gradient2(low="blue",high = "red")+geom_abline(slope = 1,intercept = 0) +
#   xlab("placebo") + ylab("drug") + ggtitle(atlasname)
#
# rm(features,drug,placebo,tstats,t_mat)
## Now run the SVM for a set of feature extraction levels.
fe_list = c(1,.9,.5,.2,.1,.05,.01)
fe_list = c(1,.5,.2,.1,.05)
fe_list = c(1,.5,.2,.1,.05)
# fe_list = c(1)
acc = matrix(nrow = length(fe_list),ncol = 5)
result_list = list()
for (n in 1:length(fe_list)) {
num <- fe_list[n]
results <- run_model(df = df,folds = 10,
feature_selection = T,feature_proportion = num,
permutation_test = F,num_permutations = 500,
type = classifier)
results[[4]]=atlasname
b <- results[[1]]
acc[n,1] <- b$estimate
acc[n,2] <- b$p.value
acc[n,3] <- num
acc[n,5] <- results[[3]] #num features
# acc[n,4] <- b$perm_p
# W <- results[[2]]
# if (is.null(dim(W))) {
#   Wmap <- write_W_connectivity(W,atlasname)
# } else if (dim(W)){
#   Wmap <- write_W_connectivity(W[1,],atlasname)
# }
# results[[5]] <- Wmap
# result_list <- c(result_list,list(results))
# # Fmap <- write_W_connectivity(W[2,],atlasname)
# c1 <- corrplot::corrplot(Wmap,is.corr = F, method = "color",na.label = "square",title = sprintf("%s W coefficients",atlasname),
#                          diag = F,type = "upper",tl.cex = .3)
# c2 <- corrplot::corrplot(Fmap,is.corr = F, method = "color",na.label = "square",title = "Feature selection counts")
# cowplot::plot_grid(c(c1,c2))
}
acc <- data.frame(accuracy = acc[,1], p.value = acc[,2], fe = acc[,3],perm.p = acc[,4],atlas = atlasname,num_features = acc[,5])
atlas_acc <- rbind(atlas_acc,acc)
write.csv(atlas_acc,file = sprintf('%s_%s_results_%s.csv',atlasname,GSR,classifier),row.names = F)
# print(ggplot(acc, aes(x = fe,y=accuracy,label=round(p.value,digits = 5))) +
# geom_point() + geom_line() + xlab("F.E. proportion") + ylab("accuracy") +geom_text()+ggtitle(atlasname))
# print(ggplot(acc, aes(x = fe,y=accuracy,label=round(perm.p,digits = 5))) +
# geom_point() + geom_line() + xlab("F.E. proportion") + ylab("accuracy") +geom_text()+ggtitle(atlasname))
}
# get critical values
n=b$parameter #number of observations
p01 <- qbinom(.01/2,size = n,prob = .5,lower.tail = F) +1 # get num correct that corresponds to two-tailed p-value of .01. Add one to get make the value represent prob of x >= p01 instead of x>p01.
p01 = p01/n #convert to proprotion
p001 <- (qbinom(.001/2,size = n,prob = .5,lower.tail = F) +1)/n
acc_plot<-ggplot(atlas_acc, aes(x = num_features,y=accuracy,color = atlas)) +
geom_point() + geom_line() + ylim(c(.5,.8))+
xlab("Num Features") + ylab("accuracy") + ggtitle(sprintf("%s %s",GSR,classifier))+
geom_hline(aes(yintercept = p01,linetype = "p = .01"))+geom_hline(aes(yintercept = p001,linetype="p = .001")) +
scale_linetype_manual(name="significance threshold",values = c(1,2))
print(acc_plot)
ggsave(filename = sprintf('%s_acc_plot_%s.svg',GSR,classifier),plot = acc_plot,device = 'svg')
write.csv(atlas_acc,file = sprintf('%s_results_%s.csv',GSR,classifier),row.names = F)
}
